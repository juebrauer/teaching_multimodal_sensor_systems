{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "In this notebook we try to classify images with a CNN.\n",
    "\n",
    "Before, we have tried to classify images with a MLP which did not work since MLP do not provide basic operations as pooling in order to foster a translation invariant representation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.3.2\n",
      "1.19.2\n",
      "4.5.1\n",
      "4.0\n",
      "2.3.0\n"
     ]
    }
   ],
   "source": [
    "import matplotlib\n",
    "print(matplotlib.__version__)\n",
    "\n",
    "import numpy as np\n",
    "print(np.__version__)\n",
    "\n",
    "import cv2\n",
    "print(cv2.__version__)\n",
    "\n",
    "import pickle\n",
    "print(pickle.format_version)\n",
    "\n",
    "import tensorflow\n",
    "print(tensorflow.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load in the image dataset generated before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "fname = \"../01_cnn_generating_image_data/persons_icons_dataset.pkl\"\n",
    "\n",
    "fobj = open(fname, \"rb\")\n",
    "dataset = pickle.load(fobj)\n",
    "fobj.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_imgs, list_labels = dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "500"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(list_imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "500"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(list_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.]]),\n",
       " array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.]])]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_imgs[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 2]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_labels[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.imshow(list_imgs[0], cmap=\"gray\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label = list_labels[0]\n",
    "label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 4)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "min(list_labels), max(list_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nr_object_classes = max(list_labels)+1\n",
    "nr_object_classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare input and output matrices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For training a MLP, where the inputs are 1-dimensional we need a 2D-matrix (for batch-learning) in Keras: in each row of this 2D-matrix there was a single input vector stored.\n",
    "\n",
    "For training a CNN with color images, you need a 4-dimensional input tensor in Keras!\n",
    "\n",
    "Here is what each of these 4 dimensions is used for:\n",
    "\n",
    "    dim #1: sample nr\n",
    "    dim #2: height of the image\n",
    "    dim #3: width of the image\n",
    "    dim #4: depth of the image (e.g. 3 color channels; RGB)\n",
    "    \n",
    "OK. So let us prepare a 4D input tensor!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_img = list_imgs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(example_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(300, 300)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_img.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "500"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nr_images = len(list_imgs)\n",
    "nr_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_height = example_img.shape[0]\n",
    "img_width  = example_img.shape[1]\n",
    "img_depth  = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.zeros((nr_images, img_height, img_width, img_depth))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(500, 300, 300, 1)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "45000000"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There it is! A 4D input tensor. But up to now only zeros...\n",
    "\n",
    "The output tensor on the other hand is still a 2D output tensor: in each row we store for each of the sample images a one-hot encoded vector that encodes which object class is present."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = np.zeros((nr_images, nr_object_classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_one_hot_vec(length, pos):\n",
    "    vec = np.zeros(length)\n",
    "    vec[pos] = 1\n",
    "    return vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 1.])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_vec = get_one_hot_vec(nr_object_classes, 4)\n",
    "test_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "for img_nr in range(0, nr_images):\n",
    "    \n",
    "    # get the next image\n",
    "    img = list_imgs[img_nr]\n",
    "        \n",
    "    # save image in 4D input tensor\n",
    "    X[img_nr,:,:,0] = img\n",
    "    \n",
    "    # get label (0,1,2,3,or 4)\n",
    "    label = list_labels[img_nr]\n",
    "    \n",
    "    # save label as one-hot encoded vector    \n",
    "    Y[img_nr,:] = get_one_hot_vec(nr_object_classes, label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split image data into train and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "500"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "N = len(X)\n",
    "N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "250"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "half = int(N/2)\n",
    "half"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X[:half]\n",
    "Y_train = Y[:half]\n",
    "\n",
    "X_test  = X[half:]\n",
    "Y_test  = Y[half:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(250, 300, 300, 1)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(250, 300, 300, 1)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "250"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "250"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build the CNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 298, 298, 32)      320       \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 149, 149, 32)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 147, 147, 64)      18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 73, 73, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 71, 71, 128)       73856     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 35, 35, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 33, 33, 256)       295168    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 16, 16, 256)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 14, 14, 512)       1180160   \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, 7, 7, 512)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 5, 5, 1024)        4719616   \n",
      "_________________________________________________________________\n",
      "max_pooling2d_5 (MaxPooling2 (None, 2, 2, 1024)        0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 4096)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 10)                40970     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 5)                 55        \n",
      "=================================================================\n",
      "Total params: 6,328,641\n",
      "Trainable params: 6,328,641\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras import losses\n",
    "\n",
    "K.clear_session()\n",
    "\n",
    "model = Sequential()\n",
    "model.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(img_height, img_width, 1)))\n",
    "model.add(layers.MaxPooling2D((2,2)))\n",
    "\n",
    "model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "model.add(layers.Conv2D(128, (3, 3), activation='relu'))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "model.add(layers.Conv2D(256, (3, 3), activation='relu'))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "model.add(layers.Conv2D(512, (3, 3), activation='relu'))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "model.add(layers.Conv2D(1024, (3, 3), activation='relu'))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "model.add(layers.Flatten())\n",
    "\n",
    "model.add(layers.Dense(10, activation='relu'))\n",
    "nr_output_neurons = nr_object_classes\n",
    "model.add(layers.Dense(nr_output_neurons))\n",
    "\n",
    "model.compile(loss=losses.mean_squared_error, optimizer='sgd')\n",
    "#model.compile(loss=losses.categorical_crossentropy, optimizer='sgd')\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test the model before training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(model, X_test, Y_test, show_some_predictions=False):\n",
    "    preds = model.predict(X_test)\n",
    "    \n",
    "    nr_test_samples = Y_test.shape[0]\n",
    "    correct = 0\n",
    "    for test_nr in range(0,nr_test_samples):        \n",
    "        should_label = np.argmax(Y_test[test_nr])\n",
    "        is_label     = np.argmax(preds[test_nr])\n",
    "        \n",
    "        if should_label==is_label:\n",
    "            correct += 1\n",
    "        \n",
    "        if show_some_predictions and test_nr<10:\n",
    "            print(\"Should: {0} vs. Is: {1}\".format(Y_test[test_nr], preds[test_nr]))\n",
    "            print(\"Should label: {0} vs. Is label: {1}\\n\".format(should_label, is_label))            \n",
    "            \n",
    "    correctly_classified_percent = (correct / nr_test_samples) * 100.0\n",
    "    print(\"Correctly classified: {0:.2f}\".format(correctly_classified_percent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Should: [0. 0. 0. 1. 0.] vs. Is: [-5.4774620e-03  5.3034495e-03 -6.0838880e-05 -1.8826185e-03\n",
      " -1.0619825e-04]\n",
      "Should label: 3 vs. Is label: 1\n",
      "\n",
      "Should: [0. 0. 1. 0. 0.] vs. Is: [ 0.00055864  0.00330172  0.00244014  0.00269991 -0.00412536]\n",
      "Should label: 2 vs. Is label: 1\n",
      "\n",
      "Should: [0. 0. 0. 1. 0.] vs. Is: [-0.00699693 -0.00101437  0.00223803 -0.00044407  0.00107737]\n",
      "Should label: 3 vs. Is label: 2\n",
      "\n",
      "Should: [0. 0. 0. 0. 1.] vs. Is: [-0.00788497 -0.00442901  0.00418719  0.00050726  0.00174314]\n",
      "Should label: 4 vs. Is label: 2\n",
      "\n",
      "Should: [0. 1. 0. 0. 0.] vs. Is: [-0.00923243 -0.00073998 -0.00075261  0.00570346 -0.0056887 ]\n",
      "Should label: 1 vs. Is label: 3\n",
      "\n",
      "Should: [1. 0. 0. 0. 0.] vs. Is: [-1.1237640e-05  5.3339703e-05  2.6718342e-05  9.4769588e-05\n",
      " -1.2087415e-04]\n",
      "Should label: 0 vs. Is label: 3\n",
      "\n",
      "Should: [0. 0. 1. 0. 0.] vs. Is: [-0.01241657  0.01548785  0.00322812 -0.00516478  0.00235051]\n",
      "Should label: 2 vs. Is label: 1\n",
      "\n",
      "Should: [0. 0. 1. 0. 0.] vs. Is: [ 0.00018904  0.00428295  0.00573752  0.0141678  -0.01656509]\n",
      "Should label: 2 vs. Is label: 3\n",
      "\n",
      "Should: [0. 1. 0. 0. 0.] vs. Is: [-0.01555631 -0.0054674   0.00417186  0.0014485  -0.00304929]\n",
      "Should label: 1 vs. Is label: 2\n",
      "\n",
      "Should: [0. 0. 1. 0. 0.] vs. Is: [-0.00785857  0.01141023 -0.00111585 -0.00425515 -0.00098385]\n",
      "Should label: 2 vs. Is label: 1\n",
      "\n",
      "Correctly classified: 20.00\n"
     ]
    }
   ],
   "source": [
    "test_model(model, X_test, Y_test, show_some_predictions=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Should: [0. 0. 1. 0. 0.] vs. Is: [-0.00942557  0.00338418 -0.00233015  0.00465958 -0.00947453]\n",
      "Should label: 2 vs. Is label: 3\n",
      "\n",
      "Should: [0. 0. 1. 0. 0.] vs. Is: [ 0.00144648  0.00553291  0.00449104  0.00488372 -0.00738071]\n",
      "Should label: 2 vs. Is label: 1\n",
      "\n",
      "Should: [0. 0. 0. 0. 1.] vs. Is: [-0.00223974  0.00033284  0.00509808  0.00452495 -0.00199167]\n",
      "Should label: 4 vs. Is label: 2\n",
      "\n",
      "Should: [1. 0. 0. 0. 0.] vs. Is: [-0.00536783  0.00548992 -0.00071078 -0.00131314 -0.00130341]\n",
      "Should label: 0 vs. Is label: 1\n",
      "\n",
      "Should: [1. 0. 0. 0. 0.] vs. Is: [-0.00226216 -0.00034117  0.00444888  0.00321495 -0.00272081]\n",
      "Should label: 0 vs. Is label: 2\n",
      "\n",
      "Should: [1. 0. 0. 0. 0.] vs. Is: [-1.2027927e-02 -1.8565720e-03 -8.5653039e-05  6.3862931e-03\n",
      " -6.4612813e-03]\n",
      "Should label: 0 vs. Is label: 3\n",
      "\n",
      "Should: [0. 0. 0. 0. 1.] vs. Is: [-0.00400838  0.00634596  0.0110302   0.01262049 -0.01192867]\n",
      "Should label: 4 vs. Is label: 3\n",
      "\n",
      "Should: [0. 0. 1. 0. 0.] vs. Is: [-5.2910123e-05  3.2206436e-04  1.9534180e-04  5.4148387e-04\n",
      " -7.1521918e-04]\n",
      "Should label: 2 vs. Is label: 3\n",
      "\n",
      "Should: [0. 0. 0. 1. 0.] vs. Is: [-0.0117606   0.02546423  0.00896036  0.00473131 -0.01270413]\n",
      "Should label: 3 vs. Is label: 1\n",
      "\n",
      "Should: [0. 1. 0. 0. 0.] vs. Is: [-0.01062117 -0.00529458  0.0029269   0.00325078 -0.00130592]\n",
      "Should label: 1 vs. Is label: 3\n",
      "\n",
      "Correctly classified: 20.00\n"
     ]
    }
   ],
   "source": [
    "test_model(model, X_train, Y_train, show_some_predictions=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datetime.datetime(2021, 5, 15, 12, 31, 35, 338522)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "datetime.now()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "8/8 [==============================] - 14s 2s/step - loss: 0.1981\n",
      "Epoch 2/500\n",
      "8/8 [==============================] - 14s 2s/step - loss: 0.1941\n",
      "Epoch 3/500\n",
      "8/8 [==============================] - 14s 2s/step - loss: 0.1905\n",
      "Epoch 4/500\n",
      "8/8 [==============================] - 13s 2s/step - loss: 0.1871\n",
      "Epoch 5/500\n",
      "8/8 [==============================] - 13s 2s/step - loss: 0.1842\n",
      "Epoch 6/500\n",
      "4/8 [==============>...............] - ETA: 5s - loss: 0.1819"
     ]
    }
   ],
   "source": [
    "datetime.now()\n",
    "history = model.fit(X_train,Y_train, epochs=500, verbose=1)\n",
    "datetime.now()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(history.history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history[\"loss\"])\n",
    "plt.xlabel(\"epoch\", fontsize=15)\n",
    "plt.ylabel(\"loss\", fontsize=15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test the model after training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_model(model, X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_model(model, X_test, Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
